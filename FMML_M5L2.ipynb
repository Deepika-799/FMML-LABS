{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepika-799/FMML-LABS/blob/main/FMML_M5L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x9mXnIyH_TU"
      },
      "source": [
        "# Lab 2\n",
        "# Classification II : Introduction to Decision Trees\n",
        "\n",
        "```\n",
        "Module Coordinator : Nikunj Nawal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of some property by inferring simple decision rules from the data features.\n",
        "\n",
        "\n",
        "Let us take a look at an example of a decision tree which predicts the class of the species of Iris flower from the iris dataset\n"
      ],
      "metadata": {
        "id": "lA8j3sciushf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRLg2DwJTqMN"
      },
      "source": [
        "#Importing the necessary packages\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8eLU5YOhiPH"
      },
      "source": [
        "### Code for the core experiment:\n",
        "\n",
        "- Creating the decision tree classifier based on parameters passed.\n",
        "- Evaluating the classifier's accuracy and plotting its confusion matrix.\n",
        "- Plotting its decision boundary.\n",
        "- Creating and showing the visualization of the tree made.\n",
        "\n",
        "**SKIP THE CODE IN THE FOLLOWING CELL FOR NOW AND COME BACK TO IT LATER AFTER UNDERSTANDING THE IDEA AND INTUITION BEHIND DECISION TREES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNo2x3Tbhhu3"
      },
      "source": [
        "def performExperiment(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  #Importing the Decision tree classifier from sklearn:\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))\n",
        "\n",
        "  print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "  fig, ax = plt.subplots(figsize=(12,10))\n",
        "  imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 12)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def giveAnExample(n : int):\n",
        "  performExperiment((X_train, y_train),  (X_test, y_test), feature_names = iris[\"feature_names\"], class_names = iris[\"target_names\"], max_depth = n)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  plt.title(\"Decision Boundary for two features used in Decision Tree\")\n",
        "  # plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsxUs3oYbFNK"
      },
      "source": [
        "## Loading IRIS Dataset:\n",
        "\n",
        "### About the IRIS dataset:\n",
        "\n",
        "The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). We shall be using decision trees to try to predict the correct species of the flower using these four features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VR4gNQ5Vuwk"
      },
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "irisData = pandas.DataFrame(\\\n",
        "    data = np.hstack((X,y.reshape(y.shape[0], 1), [[iris[\"target_names\"][int(classIdx)]] for classIdx in y])), \\\n",
        "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', \"Class\", \"ClassName\"])\n",
        "irisData.sample(n = 10, random_state = 1)\n",
        "\n",
        "#Here is a few samples: The dataset has 4 non-catagorical features and a class which can take of one of the three values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oN3nv1ExEcJ"
      },
      "source": [
        "## Example of DT on Iris dataset with performace evaluation, and tree structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxwk-zEuqc9I"
      },
      "source": [
        "giveAnExample(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkrm75u1x9RX"
      },
      "source": [
        "### Task 1:\n",
        "Use the above tree to evaluate the classes for the following examples and find the accuracy over these 5 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31oDgYlw2cg"
      },
      "source": [
        "irisData.sample(n = 5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afEhTjh9jCT2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjTv9DSA1zbg"
      },
      "source": [
        "Now let us see how we perform when we try to have a more complex decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKqh4_BzB4K"
      },
      "source": [
        "giveAnExample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7wSLX5zTIr"
      },
      "source": [
        "### Task 2:\n",
        "Repeat Exercise 1 for the above tree as well.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We observe that even though that the tree had four features available to it, the tree uses only two of them to classify the cases of species. It gives us an idea that those two features chosen are performing quite decently. Let us examine the decision boundary generated by the tree when only those two features namely **petal length and petal width** are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJN66PO-4Q0h"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [2, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [2, 3], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQ6VO7J78vp"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKnuGzgf6eoZ"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fVoKPWL8W-B"
      },
      "source": [
        "**Decision boundary** with considering **sepal length and pedal length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSDC6Il7wDA"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 2]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKiQd5Y8ox5"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and pedal width**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6StxQVn8i01"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [1, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uuefe-k_dAO"
      },
      "source": [
        "---\n",
        "\n",
        "### Task 3:\n",
        "\n",
        "#### 3.1 :\n",
        "We see that the above decision boundaries are with depth of 3. Compare the above boundary with trees that have higher complexity (by changing the value of `max_depth`) and report your observations. also compare the accuracies for different values of max_depth\n",
        "\n",
        "Test with `max_depth` of the following values:\n",
        "- 2\n",
        "- 5\n",
        "- 8\n",
        "- 10\n",
        "\n",
        "#### 3.2 :\n",
        "\n",
        "On a closer look, we see that the decision boundaries' lines are always at a right angle to the principle axes. Can you reason on why is that the case? \\\n",
        "`(Hint: How is a decision made at any node?)`\n",
        "\n",
        "---\n",
        "\n",
        "### Task 4:\n",
        "\n",
        "#### 4.1 :\n",
        "Complete the following function predict: which takes in four variables : `sepal width, sepal length, petal width, petal length` and returns the class of the flower.\n",
        "\n",
        "#### 4.2 :\n",
        "Use the decision tree made in Exercise 2 and report the logic using multiple nested `if else` statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAIeeEFY84oA"
      },
      "source": [
        "def predictSpecies(sepal_width, sepal_length, petal_width,  petal_length) -> str :\n",
        "  \"\"\"\n",
        "    Write your program here to return the species of the plant (string) using if else statements.\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4agDMEMCNeB-"
      },
      "source": [
        "# Entropy and Information:\n",
        "\n",
        "## How are decision trees built?\n",
        "\n",
        "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).\n",
        "We use entropy to calculate the homogeneity of a sample.\n",
        "\n",
        "Entropy itself is defined in the following way:\n",
        "\n",
        "$$E(s) = \\sum_{i=1}^c - p_i * log_2(p_i)$$\n",
        "\n",
        "Where $i$ iterates through the classes of the current group and $p_i$ is the probability of choosing an item from class $i$ when a datapoint is randomly picked from the group.\n",
        "\n",
        "At anypoint in the process of making the decision tree. All possible methods of dividing the group are considered (across all features and values of separations) and then the division with the most amount of **Information Gain** is used to divide the current group into two. This is done recursively to finally attain a tree.\n",
        "\n",
        "Here Information Gain is defined by the difference in Entropy of the group before the division and the weighted sum of the entropy of the two groups after division.\n",
        "\n",
        "$$IG(X) = E(s) - E(s, X)$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnd4qhrM1Km"
      },
      "source": [
        "irisData.sample(n = 10, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMDVOKZRY1GE"
      },
      "source": [
        "## Task 5:\n",
        "Calculate the Entropy of the above collection of 10 datapoints.\n",
        "Entropy is a measure of uncertainty or impurity in the dataset. The formula for entropy is:\n",
        "\n",
        "H(S) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
        "\n",
        "Where:\n",
        "\n",
        " is the dataset,\n",
        "\n",
        " is the number of distinct classes,\n",
        "\n",
        " is the probability of class  in the dataset.\n",
        "\n",
        "\n",
        "Example Dataset:\n",
        "\n",
        "Let's assume you have the following dataset of 10 data points with two possible classes: Class A and Class B.\n",
        "\n",
        "Classes: A (6 data points), B (4 data points).\n",
        "\n",
        "Now, calculate the probabilities of each class:\n",
        "\n",
        "Now calculate the entropy:\n",
        "\n",
        "H(S) = - \\left( 0.6 \\log_2(0.6) + 0.4 \\log_2(0.4) \\right)\n",
        "\n",
        "Let's compute that:\n",
        "\n",
        "H(S) = - \\left( 0.6 \\times (-0.736) + 0.4 \\times (-1.322) \\right)\n",
        "\n",
        "H(S) = - \\left( -0.4416 - 0.5288 \\right) = 0.9704 \\text{ bits} ]\n",
        "\n",
        "So, the entropy of this dataset is approximately 0.9704.\n",
        "\n",
        "\n",
        "## Task 6:\n",
        "#### 6.1 :\n",
        "Suggest a decision node (if, else) statement which divides the group into two groups\n",
        ".\n",
        "A decision node splits the dataset based on a feature value. For instance, if you have a feature such as \"Age\" with values below 30 and above 30, you could split the dataset using this threshold.\n",
        "\n",
        "Let's assume we split the dataset by a feature \"Age\" into two groups:\n",
        "\n",
        "Group 1: Age <= 30\n",
        "\n",
        "Group 2: Age > 30\n",
        "\n",
        "\n",
        "For each group, we would calculate the entropy and then compute the information gain.\n",
        "\n",
        "\n",
        "\n",
        "#### 6.2 :\n",
        "Also compute the Information Gain in that division step.\n",
        "Information Gain (IG) is the reduction in entropy after a split. It is calculated as:\n",
        "\n",
        "IG = H(S) - \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} H(S_i)\n",
        "\n",
        "Where:\n",
        "\n",
        " is the total number of data points,\n",
        "\n",
        " is the number of data points in subset ,\n",
        "\n",
        " is the entropy of the subset .\n",
        "\n",
        "\n",
        "For each split, you need to calculate the entropy for the new subsets and then compute the information gain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 6.3 :\n",
        "Compare this with other decision clauses that you can make and intuitively comment on which is better for classification and observe if this has any correlation with the numerical value of Information Gain.\n",
        "\n",
        "To find the best decision node, you would typically try multiple splits (based on different features) and compute the information gain for each. The split with the highest information gain is generally the best one for classification, as it reduces the uncertainty the most.\n",
        "\n",
        "Example Split:\n",
        "\n",
        "If we split by \"Age\" (<=30 or >30), compute the entropy for each of the two groups, and then compute the information gain, you would compare it with other possible splits (such as based on another feature, e.g., \"Income\" or \"Height\"). The best feature will be the one that leads to the highest information gain, meaning it provides the most clarity for classifying the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTrqJ86cYcFL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}